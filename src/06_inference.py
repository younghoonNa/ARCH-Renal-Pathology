    # -*- coding: utf-8 -*-
    """    ===========================================================================
               PATHOLOGY WSI ANALYSIS PIPELINE (End-to-End)
===========================================================================

[ê°œìš”]
ì´ íŒŒì´í”„ë¼ì¸ì€ WSI(Whole Slide Image) íŒŒì¼ì„ ì…ë ¥ë°›ì•„ Segmentation, íŠ¹ì§• ì¶”ì¶œ,
ì‚¬êµ¬ì²´ ë¶„ë¥˜(M0/M1)ë¥¼ ê±°ì³ ìµœì¢… ì§„ë‹¨ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì „ì²´ ê³µì •ì…ë‹ˆë‹¤.

[ê²½ë¡œ ì„¤ì •]
1. ì‘ì—… ë£¨íŠ¸ (ROOT_DIR): /home/khdp-user/workspace/
2. ì½”ë“œ ìœ„ì¹˜ (Script DIR): /home/khdp-user/workspace/Final_submission/
   (* ëª¨ë“  íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ 6ê°œì™€ ë©”ì¸ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì€ ì´ í´ë”ì— ìœ„ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤)
3. ëª¨ë¸ ìœ„ì¹˜ (Inference): /home/khdp-user/workspace/Final_submission/TransMIL_pt/
   (* 5ê°œì˜ foldë³„ ëª¨ë¸ íŒŒì¼ì´ ì´ í´ë” ì•ˆì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)

[ë””ë ‰í† ë¦¬ êµ¬ì¡°]
/home/khdp-user/workspace/
 |-- dataset/
 |    |-- Slide/                   <-- [ì…ë ¥] ìƒˆë¡œìš´ SVS íŒŒì¼ì„ ì—¬ê¸°ì— ë„£ìœ¼ì„¸ìš”.
 |    |-- CSV/GT_label.csv         <-- ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
 |    `-- Models/                  <-- Segmentation ëª¨ë¸ (.pt)
 |-- Annotation_3_final/           <-- [ì¤‘ê°„ì‚°ì¶œ] XML ì¢Œí‘œ (Glomerulus, IFTA)
 |-- Patch_3_final/                <-- [ì¤‘ê°„ì‚°ì¶œ] ì¼ë°˜ íŒ¨ì¹˜ H5 (20x)
 |-- Patch_3_Glom_final/           <-- [ì¤‘ê°„ì‚°ì¶œ] ì‚¬êµ¬ì²´ í¬ë¡­ H5
 |-- GigaPath/
 |    |-- ..._Normalized/          <-- [ì¤‘ê°„ì‚°ì¶œ] ì¼ë°˜ íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼
 |    |-- ..._M0M1C/               <-- [ì¤‘ê°„ì‚°ì¶œ] ì‚¬êµ¬ì²´ M0/M1 ë¶„ë¥˜ ê²°ê³¼
 |    `-- ..._Merged/              <-- [ì¤‘ê°„ì‚°ì¶œ] ìµœì¢… ë³‘í•©ëœ íŠ¹ì§• ë°ì´í„°
 `-- Final_submission/             <-- [ì‹¤í–‰ ìœ„ì¹˜]
      |-- 01_segmentation.py
      |-- 02_glom_generation.py
      |-- 03_general_features.py
      |-- 04_glom_classification.py
      |-- 05_merge_h5.py
      |-- 06_inference.py
      |-- Main_Pipeline.ipynb      <-- [ì‹¤í–‰ íŒŒì¼] ì£¼í”¼í„° ë…¸íŠ¸ë¶
      |-- infer_pred.csv           <-- [ìµœì¢…ê²°ê³¼] ìƒì„±ëœ ì˜ˆì¸¡ íŒŒì¼
      `-- TransMIL_pt/             <-- [ëª¨ë¸í´ë”] 5ê°œì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼

===========================================================================
[ì‹¤í–‰ ìˆœì„œ]
===========================================================================

Step 1. ì „ì²˜ë¦¬ ë° ì˜ì—­ ë¶„í•  (Segmentation & Patching)
   - íŒŒì¼ëª…: 01_segmentation.py
   - ê¸°  ëŠ¥: SVSì—ì„œ ì¡°ì§ì„ ì°¾ê³  UNetìœ¼ë¡œ Glomerulus/IFTA ë¶„í•  í›„ XML ë° ê¸°ë³¸ H5 ì €ì¥
   - ì…  ë ¥: dataset/Slide/*.svs
   - ì¶œ  ë ¥: Annotation_3_final/*.xml, Patch_3_final/*.h5

Step 2. ì‚¬êµ¬ì²´ ì˜ì—­ ì •ë°€ í¬ë¡­ (Glomerulus Cropping)
   - íŒŒì¼ëª…: 02_glom_generation.py
   - ê¸°  ëŠ¥: ìƒì„±ëœ XML ì¢Œí‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬êµ¬ì²´ ì´ë¯¸ì§€ë§Œ ì •ë°€í•˜ê²Œ ì˜ë¼ëƒ„
   - ì…  ë ¥: Annotation_3_final/*.xml + dataset/Slide/*.svs
   - ì¶œ  ë ¥: Patch_3_Glom_final/*.h5

Step 3. ì¼ë°˜ ì¡°ì§ íŠ¹ì§• ì¶”ì¶œ (General Feature Extraction)
   - íŒŒì¼ëª…: 03_general_features.py
   - ê¸°  ëŠ¥: Step 1ì˜ ì¼ë°˜ íŒ¨ì¹˜ë“¤ì„ GigaPath ëª¨ë¸ë¡œ ì„ë² ë”© (Macenko ì •ê·œí™” í¬í•¨)
   - ì…  ë ¥: Patch_3_final/*.h5
   - ì¶œ  ë ¥: GigaPath/x20_..._Normalized/*.h5

Step 4. ì‚¬êµ¬ì²´ ë¶„ë¥˜ ë° íŠ¹ì§• ì¶”ì¶œ (Glomerulus Classification M0/M1)
   - íŒŒì¼ëª…: 04_glom_classification.py
   - ê¸°  ëŠ¥: Step 2ì˜ ì‚¬êµ¬ì²´ íŒ¨ì¹˜ë¥¼ M0(ì •ìƒ)/M1(ê²½í™”)ë¡œ ë¶„ë¥˜í•˜ê³  ì„ë² ë”© ì €ì¥
   - ì…  ë ¥: Patch_3_Glom_final/*.h5
   - ì¶œ  ë ¥: GigaPath/x20_..._M0M1C/*.h5

Step 5. íŠ¹ì§• ë°ì´í„° ë³‘í•© (Merge Features)
   - íŒŒì¼ëª…: 05_merge_h5.py
   - ê¸°  ëŠ¥: Step 3(ì¼ë°˜ ì¡°ì§)ì™€ Step 4(ë¶„ë¥˜ëœ ì‚¬êµ¬ì²´)ì˜ H5 íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
   - ì…  ë ¥: Step 3 ì¶œë ¥ë¬¼ + Step 4 ì¶œë ¥ë¬¼
   - ì¶œ  ë ¥: GigaPath/x20_..._Merged/*.h5

Step 6. ìµœì¢… ì§„ë‹¨ ì˜ˆì¸¡ (Final Inference)
   - íŒŒì¼ëª…: 06_inference.py
   - ê¸°  ëŠ¥: ë³‘í•©ëœ íŠ¹ì§• ë°ì´í„°ë¥¼ TransMIL ëª¨ë¸ì— ë„£ì–´ ìµœì¢… í™•ë¥  ì˜ˆì¸¡
   - ì…  ë ¥: GigaPath/x20_..._Merged/*.h5, TransMIL_pt/*.pt
   - ì¶œ  ë ¥: Final_submission/infer_pred.csv

===========================================================================
[ì£¼ì˜ ì‚¬í•­]
===========================================================================
1. ê²½ë¡œ í™•ì¸: ìœ„ ROOT_DIR ê²½ë¡œê°€ ì‹¤ì œ ì„œë²„ í™˜ê²½ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.
2. GPU ì‚¬ìš©: ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” CUDA:0 (0ë²ˆ GPU) ì‚¬ìš©ì„ ê¸°ë³¸ìœ¼ë¡œ í•©ë‹ˆë‹¤.
3. íŒŒì¼ ì´ë¦„: SVS íŒŒì¼ëª…ì— ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.
4. ëª¨ë¸ í™•ì¸: Final_submission/TransMIL_pt/ í´ë” ì•ˆì— 5ê°œì˜ .pt íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.

    NOTE:
    - HuggingFace í† í°(HF_TOKEN)ì€ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
      í•„ìš”í•˜ë‹¤ë©´ ì‹¤í–‰ ì „ì— í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…í•˜ì„¸ìš”:
        export HF_TOKEN=...
    """

import os
import torch
import pandas as pd
import numpy as np
import h5py
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import math

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬(matplotlib, seaborn)ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•¨

# ============================================================
# [Settings]
# ============================================================
os.environ["HF_HUB_DISABLE_XET"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# [ìˆ˜ì •] ëª¨ë“  ê²½ë¡œì˜ ê¸°ì¤€ì´ ë˜ëŠ” Root
BASE_DIR = Path("/home/khdp-user/workspace")

# [ì ˆëŒ€ ê²½ë¡œ í™•ì¸] ì•„ë˜ ê²½ë¡œë“¤ì€ BASE_DIRì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë‘ ì ˆëŒ€ ê²½ë¡œì„
FEATURE_ROOT = BASE_DIR / "GigaPath/x20_224x224_to_224x224_Stride224v3_Normalized_Merged"
CSV_PATH = BASE_DIR / "dataset/CSV/GT_label.csv"
MODEL_ROOT_DIR = BASE_DIR / "yoongeol/MIL_run_gigapath_compare_CV/type2_TransMIL_AuxLoss_TopK_4Types_Merged"

# [ìˆ˜ì •] ê¸°ì¡´ ìƒëŒ€ ê²½ë¡œì˜€ë˜ ê²ƒì„ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½ (íŒŒì¼ ìœ„ì¹˜ê°€ workspace ë°”ë¡œ ì•„ë˜ë¼ê³  ê°€ì •)
# ë§Œì•½ íŒŒì¼ì´ dataset í´ë” ì•ˆì— ìˆë‹¤ë©´: BASE_DIR / "dataset" / "dataset_with_cluster_public_test.csv" ë¡œ ìˆ˜ì • í•„ìš”
TEST_CLUSTER_CSV = BASE_DIR / "dataset_with_cluster_public_test.csv"

# [ìˆ˜ì •] ìµœì¢… ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
FINAL_SAVE_DIR = BASE_DIR / "Final_submission"
FINAL_SAVE_PATH = FINAL_SAVE_DIR / "infer_pred.csv"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
N_FOLDS = 5

# â˜… [ì¤‘ìš”] í•™ìŠµ ì½”ë“œì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì¼í•˜ê²Œ ë§ì¶¤ â˜…
TOP_K_RATIO = 0.5     
MIN_K = 100
MAX_K = 2048
N_PATCH_CLASSES = 4 

# â˜… [ìš”ì²­ì‚¬í•­] ëª©í‘œ ì–‘ì„± ë¹„ìœ¨ 12.5% ì„¤ì • â˜…
TARGET_POS_RATIO = 0.125 

# ============================================================
# [Dataset] Inferenceìš©
# ============================================================
class GigaPathMILDataset(Dataset):
    def __init__(self, df_slide, feature_root: Path):
        self.feature_root = Path(feature_root)
        self.data_list = []
        for _, row in df_slide.iterrows():
            slide_name = str(row["SlideName"])
            h5_path = self.feature_root / f"{slide_name}.h5"
            if h5_path.exists():
                self.data_list.append({"slide_name": slide_name, "path": h5_path})
                
    def __len__(self): return len(self.data_list)
    
    def __getitem__(self, idx):
        item = self.data_list[idx]
        try:
            with h5py.File(item['path'], 'r') as f:
                feats = []
                target_keys = ['Normal', 'IFTA', 'Glomerulus', 'M0', 'M1']
                
                for cls in target_keys:
                    if cls in f:
                        e = f[cls]['emb'][:]
                        if len(e) > 0: feats.append(e)
                        
                if not feats: return torch.zeros(1,1536), item['slide_name']
                
                features = np.concatenate(feats)
                features_tensor = torch.from_numpy(features).float()
                features_norm = F.layer_norm(features_tensor, (1536,))
                
                return features_norm, item['slide_name']
        except: return torch.zeros(1,1536), item['slide_name']

# ============================================================
# [Model] TransMIL_Aux
# ============================================================
class TransMIL_Aux(nn.Module):
    def __init__(self, input_dim=1536, n_classes=2, n_patch_classes=4, top_k_ratio=0.5, min_k=100, max_k=1024):
        super(TransMIL_Aux, self).__init__()
        self.top_k_ratio = top_k_ratio
        self.min_k = min_k
        self.max_k = max_k
        
        self.pos_layer = PPEG(dim=512)
        self._fc1 = nn.Sequential(nn.Linear(input_dim, 512), nn.ReLU())
        self.cls_token = nn.Parameter(torch.randn(1, 1, 512))
        self.n_classes = n_classes
        
        self.layer1 = TransLayer(dim=512)
        self.layer2 = TransLayer(dim=512)
        self.norm = nn.LayerNorm(512)
        
        self._fc2 = nn.Linear(512, self.n_classes)
        self.aux_classifier = nn.Linear(512, n_patch_classes)

    def forward(self, x):
        patch_scores = torch.norm(x, p=2, dim=-1)
        N = x.shape[1]
        k = int(N * self.top_k_ratio)
        k = max(self.min_k, min(k, self.max_k))
        k = min(k, N)

        topk_indices = None
        if k < N and k > 0:
            _, topk_indices = torch.topk(patch_scores, k, dim=1)
            x = torch.gather(x, 1, topk_indices.unsqueeze(-1).expand(-1, -1, x.shape[-1]))
        
        h = self._fc1(x) 
        h = self.pos_layer(h, int(math.sqrt(h.shape[1]))) 
        B = h.shape[0]
        cls_tokens = self.cls_token.expand(B, -1, -1)
        h = torch.cat((cls_tokens, h), dim=1)
        h = self.layer1(h)
        h = self.layer2(h)
        h = self.norm(h)
        
        slide_logits = self._fc2(h[:, 0])
        patch_logits = self.aux_classifier(h[:, 1:])
        
        return slide_logits, patch_logits, topk_indices

class PPEG(nn.Module):
    def __init__(self, dim=512):
        super(PPEG, self).__init__()
        self.proj = nn.Conv2d(dim, dim, 7, 1, 7//2, groups=dim)
        self.proj1 = nn.Conv2d(dim, dim, 5, 1, 5//2, groups=dim)
        self.proj2 = nn.Conv2d(dim, dim, 3, 1, 3//2, groups=dim)
    def forward(self, x, H): return x 

class TransLayer(nn.Module):
    def __init__(self, norm_layer=nn.LayerNorm, dim=512):
        super().__init__()
        self.norm = norm_layer(dim)
        self.attn = NystromAttention(dim=dim, dim_head=dim//8, heads=8, num_landmarks=256)
    def forward(self, x):
        x = x + self.attn(self.norm(x))
        return x

class NystromAttention(nn.Module):
    def __init__(self, dim, dim_head, heads, num_landmarks=256):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.inner_dim = heads * dim_head
        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias = False)
        self.to_out = nn.Sequential(nn.Linear(self.inner_dim, dim), nn.Dropout(0.1))
    def forward(self, x, mask = None):
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: t.view(b, n, h, -1).transpose(1, 2), qkv)
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = dots.softmax(dim=-1)
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).reshape(b, n, -1)
        return self.to_out(out)

def load_weights(model, path):
    state_dict = torch.load(path, map_location=DEVICE)
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'): new_state_dict[k[7:]] = v
        else: new_state_dict[k] = v
    model.load_state_dict(new_state_dict)
    return model

# ============================================================
# [Main Logic] 
# ============================================================
def run_integrated_pipeline():
    # 1. ë°ì´í„° ë° ëª¨ë¸ ë¡œë“œ
    if not CSV_PATH.exists() or not TEST_CLUSTER_CSV.exists(): 
        print(f"Error: CSV file missing.\nCheck: {CSV_PATH}\nCheck: {TEST_CLUSTER_CSV}")
        return

    df = pd.read_csv(CSV_PATH)
    if 'split' in df.columns: df = df[df["split"].str.strip().str.lower() == "test"]
    elif 'GT' in df.columns: df = df[df["GT"].astype(str).str.strip().str.lower() == "test"]
    
    # [ì£¼ì˜] Cluster íŒŒì¼ì€ Test ì…‹ ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ ì‚¬ìš© (Threshold ê³„ì‚°ìš© ì•„ë‹˜)
    df_cluster = pd.read_csv(TEST_CLUSTER_CSV)
    df_test = pd.merge(df, df_cluster, left_on="SlideName", right_on="SlideName", how="inner").reset_index(drop=True)
    
    print(f"ğŸš€ Total Test Samples: {len(df_test)}")
    
    models = []
    print("\nLoading Models...")
    for fold in range(N_FOLDS):
        path = MODEL_ROOT_DIR / f"best_model_fold{fold}.pt"
        if path.exists():
            m = TransMIL_Aux(n_classes=2, n_patch_classes=N_PATCH_CLASSES, 
                             top_k_ratio=TOP_K_RATIO, min_k=MIN_K, max_k=MAX_K).to(DEVICE)
            load_weights(m, path)
            m.eval()
            models.append(m)
            print(f"Fold {fold} loaded.")
    
    if not models: print("No models loaded"); return

    # 2. Inference
    print("\n[Step 1] Running Inference...")
    ds = GigaPathMILDataset(df_test, FEATURE_ROOT)
    loader = DataLoader(ds, batch_size=1, shuffle=False, num_workers=4)
    raw_results = []
    
    with torch.no_grad():
        for x, s_names in tqdm(loader, desc="Inference"):
            x = x.to(DEVICE)
            if x.dim() == 2: x = x.unsqueeze(0)
            
            slide_probs = []
            for m in models:
                logits, _, _ = m(x) 
                prob = torch.softmax(logits, dim=1)[0, 1].item()
                slide_probs.append(prob)
                
            raw_results.append({"ID": s_names[0], "Prob": np.mean(slide_probs)})
            
    df_final = pd.DataFrame(raw_results)
    
    # --------------------------------------------------------
    # [Step 2] GLOBAL Threshold ê³„ì‚° (í†µí•© ë¹„ìœ¨)
    # --------------------------------------------------------
    all_probs = df_final["Prob"].values
    target_percentile = (1.0 - TARGET_POS_RATIO) * 100
    global_threshold = np.percentile(all_probs, target_percentile)
    
    df_final["Label_Global"] = (df_final["Prob"] >= global_threshold).astype(int)
    
    print(f"\n{'='*60}")
    print(f" ğŸŒ GLOBAL Strategy (Target Top {TARGET_POS_RATIO*100}%)")
    print(f"    Global Threshold: {global_threshold:.4f}")
    print(f"    Total Positive: {sum(df_final['Label_Global'])} / {len(df_final)} ({sum(df_final['Label_Global'])/len(df_final)*100:.1f}%)")
    print(f"{'='*60}")

    # --------------------------------------------------------
    # [Step 3] ê²°ê³¼ ì €ì¥ (Global ê¸°ì¤€ ë‹¨ì¼ íŒŒì¼)
    # --------------------------------------------------------
    # ì €ì¥ í´ë” ìƒì„±
    FINAL_SAVE_DIR.mkdir(parents=True, exist_ok=True)
    
    # ìš”ì²­í•˜ì‹  ì»¬ëŸ¼ëª… ë° í˜•ì‹ìœ¼ë¡œ ì €ì¥
    # í•„ìš”í•˜ë‹¤ë©´ ì»¬ëŸ¼ëª…ì„ ë³€ê²½: Label_Global -> Predicted_Label, Prob -> Predicted_Prob
    sub_global = df_final[["ID", "Label_Global", "Prob"]].rename(
        columns={"Label_Global": "Predicted_Label", "Prob": "Predicted_Prob"}
    )
    
    sub_global.to_csv(FINAL_SAVE_PATH, index=False)
    
    print("\n" + "="*60)
    print(f"âœ… Finished! (Ratio: {TARGET_POS_RATIO*100}%)")
    print(f"ğŸ“‚ Saved to: {FINAL_SAVE_PATH}")
    print("="*60)

if __name__ == "__main__":
    run_integrated_pipeline()
